[comment(1)]
args = text
definition = ""
iseval = 1

[confusionmatrix(2)]
args = a, p
definition = rename "$a$" as actual, "$p$" as predicted\
| stats count by actual predicted\
| appendpipe [ eval predicted=actual, count=0 ]\
| stats sum(count) as count by actual predicted\
| xyseries actual predicted count\
| rename * as "Predicted *"\
| rename "Predicted $a$" as "Actual $a$"\
| fillnull value=0
iseval = 0

[classificationreport(2)]
args = a, p
definition = rename "$a$" as actual, "$p$" as predicted\
| stats count by actual predicted\
| xyseries actual predicted count\
| fillnull\
| untable actual predicted count\
| stats sum(eval(if(actual == predicted, count, 0))) as t sum(eval(if(actual != predicted, count, 0))) as f by actual predicted\
| eventstats sum(eval(if(actual==predicted, t, 0))) as tp sum(eval(if(actual!=predicted, f, 0))) as fn by actual\
| eventstats sum(eval(if(actual!=predicted, f, 0))) as fp by predicted\
| eval count=if(actual==predicted,t,f), fp=if(actual==predicted, fp, 0), fn=if(actual==predicted, fn,0), tp=if(actual==predicted, tp, 0)\
| stats sum(count) as count sum(t) as t sum(f) as f sum(tp) as tp sum(fn) as fn sum(fp) as fp by actual\
| eventstats sum(count) as total\
| eval precision=tp/(tp+fp)\
| eval recall=tp/(tp+fn)\
| eval f1=2*precision*recall/(precision+recall)\
| eval accuracy=t/count\
| fillnull\
| appendpipe [ stats sum(count) as count sum(eval(accuracy*count/total)) as accuracy sum(eval(precision*count/total)) as precision sum(eval(recall*count/total)) as recall sum(eval(f1*count/total)) as f1 | eval actual="Weighted Average" ]\
| rename actual as class\
| table class accuracy precision recall f1 count
iseval = 0

[classificationstatistics(2)]
args = a, p
definition = `classificationreport("$a$", "$p$")`\
| tail 1\
| eval precision=round(precision, 2), recall=round(recall, 2), accuracy=round(accuracy, 2), f1=round(2*precision*recall/(precision+recall), 2)
iseval = 0
